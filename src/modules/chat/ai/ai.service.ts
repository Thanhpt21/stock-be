import { Injectable, Logger } from '@nestjs/common';
import OpenAI from 'openai';
import { PrismaService } from 'prisma/prisma.service';

interface AiOptions {
  model?: string;
  systemPrompt?: string;
  temperature?: number;
  maxTokens?: number;
}

@Injectable()
export class AiService {
  private readonly logger = new Logger(AiService.name);
  private openai: OpenAI;
  private defaultModel = 'gpt-4o-mini';
  private defaultPrompt = 'B·∫°n l√† chuy√™n gia ch·ª©ng kho√°n Vi·ªát Nam. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c v√† chuy√™n nghi·ªáp v·ªÅ th·ªã tr∆∞·ªùng ch·ª©ng kho√°n, c·ªï phi·∫øu, ph√¢n t√≠ch k·ªπ thu·∫≠t.';

  constructor(private prisma: PrismaService) {
    this.openai = new OpenAI({ 
      apiKey: process.env.OPENAI_API_KEY,
      timeout: 30000,
      maxRetries: 2,
    });
  }

  async generateReply(
    conversationHistory: { senderType: string; message: string }[],
    options?: AiOptions,
  ): Promise<string | null> {
    const systemPrompt = options?.systemPrompt || this.defaultPrompt;

    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory.map((m) => ({
        role: m.senderType === 'USER' ? 'user' : 'assistant',
        content: m.message,
      })),
    ] as OpenAI.Chat.Completions.ChatCompletionMessageParam[];

    try {
      this.logger.log('ü§ñ Calling OpenAI API...', {
        messageCount: messages.length,
        model: options?.model || this.defaultModel
      });

      const completion = await this.openai.chat.completions.create({
        model: options?.model || this.defaultModel,
        temperature: options?.temperature ?? 0.7,
        max_tokens: options?.maxTokens ?? 800,
        messages,
      });

      const content = completion.choices[0]?.message?.content?.trim();
      
      if (!content) {
        this.logger.warn('AI returned empty content');
        return 'Xin l·ªói, t√¥i ch∆∞a th·ªÉ x·ª≠ l√Ω c√¢u h·ªèi n√†y. Vui l√≤ng th·ª≠ l·∫°i.';
      }

      this.logger.log('‚úÖ AI response generated', {
        length: content.length,
        preview: content.substring(0, 100)
      });

      return content;

    } catch (error) {
      this.logger.error('‚ùå AI generateReply error:', error);
      
      // Fallback responses based on error type
      if (error.code === 'insufficient_quota') {
        return 'Hi·ªán t·∫°i d·ªãch v·ª• AI ƒëang b·∫£o tr√¨. Vui l√≤ng th·ª≠ l·∫°i sau.';
      } else if (error.code === 'rate_limit_exceeded') {
        return 'H·ªá th·ªëng ƒëang qu√° t·∫£i. Vui l√≤ng ƒë·ª£i m·ªôt ch√∫t v√† th·ª≠ l·∫°i.';
      } else {
        return 'Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau.';
      }
    }
  }
}